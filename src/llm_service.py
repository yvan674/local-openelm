#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2024 Apple Inc. All Rights Reserved.
#

"""Module to generate OpenELM output given a model and an input prompt."""
import os
import logging
import time
import argparse
from pathlib import Path
from typing import Optional, Union

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.generation.streamers import BaseStreamer


HF_ACCESS_TOKEN = os.environ["HF_ACCESS_TOKEN"]


class TokenStreamer(BaseStreamer):
    """Handles token streaming from the model's generate method. """
    def __init__(self):
        self.generated_tokens = []

    def put(self, value):
        """ Stores each token generated by the model. """
        self.generated_tokens.append(value)

    def end(self):
        """ Indicates the end of generation process. """
        print("Generation ended.")  # Optional: signal end of generation

    def __iter__(self):
        """ Allow the streamer to be iterable. """
        return iter(self.generated_tokens)


class OpenELM:
    def __init__(self,
                 model_fp: Path,
                 assistant_model_fp: Path = None,
                 device: torch.device = None):
        """Interface with a model with defaults set up correctly.

        Args:
            model_fp: Path to the model checkpoints dir.
            assistant_model_fp: Path to the assistant model's checkpoint dir,
                if an assistant model is desired.
            device: The torch device to use. Automatically chosen if None.
        """
        self.model = AutoModelForCausalLM.from_pretrained(
            model_fp,
            trust_remote_code=True
        )
        self.model.to(device).eval()

        if assistant_model_fp is None:
            self.assistant_model = None
        else:
            self.assistant_model = AutoModelForCausalLM.from_pretrained(
                assistant_model_fp,
                trust_remote_code=True
            )
            self.assistant_model.to(device).eval()

        self.tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            token=HF_ACCESS_TOKEN
        )

        self.device = device if device is not None else self._get_device()

    @staticmethod
    def _get_device():
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")

    def stream_generate(self,
                        prompt: str,
                        max_length: int = 1024):
        """Generates output given a prompt using OpenELM.

        Args:
            prompt: The string prompt.
            max_length: The maximum generation length of tokens. This is input
            prompt + generated tokens
        """
        tokenized_prompt = self.tokenizer(prompt)
        tokenized_prompt = torch.tensor(
            tokenized_prompt["input_ids"],
            device=self.device
        )
        tokenized_prompt = tokenized_prompt.unsqueeze(0)

        streamer = TokenStreamer()

        self.model.generate(tokenized_prompt,
                            max_length=max_length,
                            pad_token_id=0,
                            streamer=streamer)

        for token_id in streamer:
            word = self.tokenizer.decode(
                [token_id], skip_special_tokens=True,
            )
            yield word


def generate(
    prompt: str,
    model: Union[str, AutoModelForCausalLM],
    hf_access_token: str = None,
    tokenizer: Union[str, AutoTokenizer] = 'meta-llama/Llama-2-7b-hf',
    device: Optional[str] = None,
    max_length: int = 1024,
    assistant_model: Optional[Union[str, AutoModelForCausalLM]] = None,
    generate_kwargs: Optional[dict] = None,
) -> tuple[str, any]:
    """ Generates output given a prompt.

    Args:
        prompt: The string prompt.
        model: The LLM Model. If a string is passed, it should be the path to
            the hf converted checkpoint.
        hf_access_token: Hugging face access token.
        tokenizer: Tokenizer instance. If model is set as a string path,
            the tokenizer will be loaded from the checkpoint.
        device: String representation of device to run the model on. If None
            and cuda available it would be set to cuda:0 else cpu.
        max_length: Maximum length of tokens, input prompt + generated tokens.
        assistant_model: If set, this model will be used for
            speculative generation. If a string is passed, it should be the
            path to the hf converted checkpoint.
        generate_kwargs: Extra kwargs passed to the hf generate function.

    Returns:
        output_text: output generated as a string.
        generation_time: generation time in seconds.

    Raises:
        ValueError: If device is set to CUDA but no CUDA device is detected.
        ValueError: If tokenizer is not set.
        ValueError: If hf_access_token is not specified.
    """
    if not device:
        if torch.cuda.is_available() and torch.cuda.device_count():
            device = "cuda:0"
            logging.warning(
                'inference device is not set, using cuda:0, %s',
                torch.cuda.get_device_name(0)
            )
        elif torch.backends.mps.is_available():
            device = "mps"
            logging.warning(
                "inference device is not set, using mps"
            )
        else:
            device = 'cpu'
            logging.warning(
                (
                    'No CUDA device detected, using cpu, '
                    'expect slower speeds.'
                )
            )

    if 'cuda' in device and not torch.cuda.is_available():
        raise ValueError('CUDA device requested but no CUDA device detected.')

    if not tokenizer:
        raise ValueError('Tokenizer is not set in the generate function.')

    if not hf_access_token:
        raise ValueError((
            'Hugging face access token needs to be specified. '
            'Please refer to https://huggingface.co/docs/hub/security-tokens'
            ' to obtain one.'
            )
        )

    if isinstance(model, str):
        checkpoint_path = model
        model = AutoModelForCausalLM.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
    model.to(device).eval()
    if isinstance(tokenizer, str):
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer,
            token=hf_access_token,
        )

    # Speculative mode
    draft_model = None
    if assistant_model:
        draft_model = assistant_model
        if isinstance(assistant_model, str):
            draft_model = AutoModelForCausalLM.from_pretrained(
                assistant_model,
                trust_remote_code=True
            )
        draft_model.to(device).eval()

    # Prepare the prompt
    chat = [
        {"role": "system",
         "content": "You are a friendly assistant."},
        {"role": "user",
         "content": prompt}
    ]
    tokenized_prompt = tokenizer.apply_chat_template(chat)
    tokenized_prompt = torch.tensor(
        tokenized_prompt,
        device=device
    )

    tokenized_prompt = tokenized_prompt.unsqueeze(0)

    # Generate
    stime = time.time()
    output_ids = model.generate(
        tokenized_prompt,
        max_length=max_length,
        pad_token_id=0,
        assistant_model=draft_model,
        **(generate_kwargs if generate_kwargs else {}),
    )
    generation_time = time.time() - stime

    output_text = tokenizer.decode(
        output_ids[0].tolist(),
        skip_special_tokens=True
    )

    return output_text, generation_time


if __name__ == '__main__':
    # _m = OpenELM(Path("/Users/Yvan/Git/OpenELM-270M-Instruct"))
    #
    # for token in _m.stream_generate("Hello there!"):
    #     print(token, sep="")
    _out, _g_time = generate(
        "Hi there, how are you?",
        str(Path("/Users/Yvan/Git/OpenELM-270M-Instruct")),
        HF_ACCESS_TOKEN,
        device="cpu",
        max_length=64
    )

    print("Response:")
    print(_out)
    print(f"Generation time: {_g_time}")
